<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-09-16T23:48:15+08:00</updated><id>http://localhost:4000/</id><title type="html">Xiaoyu Zhang</title><subtitle>This is the personal website of Xiaoyu Zhang</subtitle><author><name>Xiaoyu Zhang</name></author><entry><title type="html">Lattice-based path planner within baidu Apollo framework</title><link href="http://localhost:4000/baidu_apollo.html" rel="alternate" type="text/html" title="Lattice-based path planner within baidu Apollo framework" /><published>2017-04-20T15:52:31+08:00</published><updated>2017-04-20T15:52:31+08:00</updated><id>http://localhost:4000/baidu_apollo</id><content type="html" xml:base="http://localhost:4000/baidu_apollo.html">&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row-1&quot;&gt;
        &lt;!-- &lt;div class=&quot;col-md-8&quot;&gt; --&gt;
            &lt;p&gt;
            Proposed a novel simple-based planning algorithm for non-holonomic constrained robot to pass through environments with narrow passages. Dividing the configuration space into holonomic set and non-holonomic set, the algorithm converts the original globally-constrained planning problem into a series of locally constrained problems. Then by combining the regional solutions, the algorithm output a globally feasible path. 
            &lt;/p&gt;
            &lt;p&gt;
            The overview of our algorithm appears in Algorithm：
            &lt;/p&gt;
            &lt;ul&gt;
                &lt;li&gt;
                   RRT planning in holonomic space.
                &lt;/li&gt;
                &lt;li&gt;
                   Sampling narrow configurations with maximum margin in narrow passage.
                &lt;/li&gt;
                &lt;li&gt;
                   Identifying escape velocity for each narrow configuration.
                &lt;/li&gt;
                &lt;li&gt;
                   Controller based dual-direction planning with nonholonomic constraints.
                &lt;/li&gt;

            &lt;/ul&gt;
        &lt;!-- &lt;/div&gt; --&gt;
    &lt;/div&gt;
        
&lt;/div&gt;

&lt;div class=&quot;col-md-4&quot;&gt;
            &lt;video class=&quot;embed-responsive-item&quot; controls autoplay muted&gt;
                &lt;source src=&quot;/assets/posts_img/kinodynamic_aggressive_trajectory_planner/kat_demo.mp4&quot; type=&quot;video/mp4&quot;&gt;
                  Your browser does not support the video tag.
            &lt;/video&gt;
&lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-4&quot;&gt;
            &lt;div class=&quot;card mb-4 shadow-sm project-img&quot;&gt;
                &lt;img class=&quot;card-img-top&quot; src=&quot;/assets/posts_img/kinodynamic_aggressive_trajectory_planner/quad.png&quot; alt=&quot;Card image cap&quot;&gt;
            &lt;/div&gt;
    &lt;/div&gt;
        &lt;div class=&quot;col-md-8&quot;&gt;
            &lt;div class=&quot;card mb-4 shadow-sm project-img&quot;&gt;
                &lt;img class=&quot;card-img-top&quot; src=&quot;/assets/posts_img/kinodynamic_aggressive_trajectory_planner/wholetra.png&quot; alt=&quot;Card image cap&quot;&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

&lt;/div&gt;</content><author><name>Xiaoyu Zhang</name></author><category term="Robotics" /><summary type="html">Proposed a novel simple-based planning algorithm for non-holonomic constrained robot to pass through environments with narrow passages. Dividing the configuration space into holonomic set and non-holonomic set, the algorithm converts the original globally-constrained planning problem into a series of locally constrained problems. Then by combining the regional solutions, the algorithm output a globally feasible path. The overview of our algorithm appears in Algorithm： RRT planning in holonomic space. Sampling narrow configurations with maximum margin in narrow passage. Identifying escape velocity for each narrow configuration. Controller based dual-direction planning with nonholonomic constraints.</summary></entry><entry><title type="html">Kinodynamic Aggressive Trajectory Planner</title><link href="http://localhost:4000/kat_planner.html" rel="alternate" type="text/html" title="Kinodynamic Aggressive Trajectory Planner" /><published>2017-04-20T15:52:31+08:00</published><updated>2017-04-20T15:52:31+08:00</updated><id>http://localhost:4000/kinodynamic_aggressive_trajectory_planner</id><content type="html" xml:base="http://localhost:4000/kat_planner.html">&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row-1&quot;&gt;
        &lt;!-- &lt;div class=&quot;col-md-8&quot;&gt; --&gt;
            &lt;p&gt;
            Proposed a novel simple-based planning algorithm for non-holonomic constrained robot to pass through environments with narrow passages. Dividing the configuration space into holonomic set and non-holonomic set, the algorithm converts the original globally-constrained planning problem into a series of locally constrained problems. Then by combining the regional solutions, the algorithm output a globally feasible path. 
            &lt;/p&gt;
            &lt;p&gt;
            The overview of our algorithm appears in Algorithm：
            &lt;/p&gt;
            &lt;ul&gt;
                &lt;li&gt;
                   RRT planning in holonomic space.
                &lt;/li&gt;
                &lt;li&gt;
                   Sampling narrow configurations with maximum margin in narrow passage.
                &lt;/li&gt;
                &lt;li&gt;
                   Identifying escape velocity for each narrow configuration.
                &lt;/li&gt;
                &lt;li&gt;
                   Controller based dual-direction planning with nonholonomic constraints.
                &lt;/li&gt;

            &lt;/ul&gt;
        &lt;!-- &lt;/div&gt; --&gt;
    &lt;/div&gt;
        
&lt;/div&gt;

&lt;div class=&quot;col-md-4&quot;&gt;
            &lt;video class=&quot;embed-responsive-item&quot; controls autoplay muted&gt;
                &lt;source src=&quot;/assets/posts_img/kinodynamic_aggressive_trajectory_planner/kat_demo.mp4&quot; type=&quot;video/mp4&quot;&gt;
                  Your browser does not support the video tag.
            &lt;/video&gt;
&lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-4&quot;&gt;
            &lt;div class=&quot;card mb-4 shadow-sm project-img&quot;&gt;
                &lt;img class=&quot;card-img-top&quot; src=&quot;/assets/posts_img/kinodynamic_aggressive_trajectory_planner/quad.png&quot; alt=&quot;Card image cap&quot;&gt;
            &lt;/div&gt;
    &lt;/div&gt;
        &lt;div class=&quot;col-md-8&quot;&gt;
            &lt;div class=&quot;card mb-4 shadow-sm project-img&quot;&gt;
                &lt;img class=&quot;card-img-top&quot; src=&quot;/assets/posts_img/kinodynamic_aggressive_trajectory_planner/wholetra.png&quot; alt=&quot;Card image cap&quot;&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;

&lt;/div&gt;</content><author><name>Xiaoyu Zhang</name></author><category term="Robotics" /><summary type="html">Proposed a novel simple-based planning algorithm for non-holonomic constrained robot to pass through environments with narrow passages. Dividing the configuration space into holonomic set and non-holonomic set, the algorithm converts the original globally-constrained planning problem into a series of locally constrained problems. Then by combining the regional solutions, the algorithm output a globally feasible path. The overview of our algorithm appears in Algorithm： RRT planning in holonomic space. Sampling narrow configurations with maximum margin in narrow passage. Identifying escape velocity for each narrow configuration. Controller based dual-direction planning with nonholonomic constraints.</summary></entry><entry><title type="html">PR2_robot_motion_planning</title><link href="http://localhost:4000/PR2_robot_motion_planning.html" rel="alternate" type="text/html" title="PR2_robot_motion_planning" /><published>2016-05-10T15:52:31+08:00</published><updated>2016-05-10T15:52:31+08:00</updated><id>http://localhost:4000/PR2_robot_motion_planning</id><content type="html" xml:base="http://localhost:4000/PR2_robot_motion_planning.html">&lt;div class=&quot;container&quot;&gt;
	&lt;p&gt;
	Developed a navigation and manipulation planner for PR2 robot that can navigate to goal pose and move one of its end-effector to given position in the given environment.
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;container&quot;&gt;
	&lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-md-12&quot;&gt;
        &lt;table&gt;
        	&lt;tr&gt;
        		&lt;td valign=&quot;top&quot;&gt; 
			        &lt;div class=&quot;col-12&quot;&gt;
			            &lt;p&gt;
			            Implemented the biRRT-Connect algorithm to search the collision-free paths for the left arm of the PR2 robot in configuration space from the current configuration to a goal configuration, where object located.
			            &lt;/p&gt;
			            &lt;p&gt;
			            In this project 
			            &lt;/p&gt;
			            &lt;ul&gt;
			                &lt;li&gt;
			                    Developed motion planning based on RRT
			                &lt;/li&gt;
			                &lt;li&gt;
			                    Developed motion planning based on biRRT. 
			                &lt;/li&gt;

			            &lt;/ul&gt;
			        &lt;/div&gt;
				&lt;/td&gt;
        		&lt;td&gt;
		        	&lt;video width=&quot;400&quot; height=&quot;300&quot; controls autoplay muted&gt;
		              &lt;source src=&quot;/assets/posts_img/PR2_robot_motion_planning/pr2_rrt_demo.mp4&quot; type=&quot;video/mp4&quot;&gt;
		              Your browser does not support the video tag.
		            &lt;/video&gt;
        		&lt;/td&gt;
    		&lt;/tr&gt;
		&lt;/table&gt;
                 
	&lt;/div&gt;
&lt;/div&gt;


&lt;div class=&quot;container&quot;&gt;
	&lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-md-12&quot;&gt;
        &lt;table&gt;
        	&lt;tr&gt;
        		&lt;td valign=&quot;top&quot;&gt; 
			        &lt;div class=&quot;col-12&quot;&gt;
			            &lt;p&gt;
			            Implemented the A*/ANA* path planning algorithm to find the shortest collision-free path for the PR2’s base from the start pose to the goal pose.
			            &lt;/p&gt;
			            &lt;p&gt;
			            In this project 
			            &lt;/p&gt;
			            &lt;ul&gt;
			            	&lt;li&gt;
			                    Implemented ANA* (Anytime Non-parameteric A*)
			                &lt;/li&gt;
			                &lt;li&gt;
			                    Tested 4-connect and 8-connect
			                &lt;/li&gt;
			                &lt;li&gt;
			                    Tested different heuristic function, Manhattan distance and euclidean distance.
			                &lt;/li&gt;

			            &lt;/ul&gt;
			        &lt;/div&gt;
				&lt;/td&gt;
        		&lt;td&gt;
		        	&lt;video width=&quot;400&quot; height=&quot;300&quot; controls autoplay muted&gt;
		              &lt;source src=&quot;/assets/posts_img/PR2_robot_motion_planning/pr2_astar_demo.mp4&quot; type=&quot;video/mp4&quot;&gt;
		              Your browser does not support the video tag.
		            &lt;/video&gt;
        		&lt;/td&gt;
    		&lt;/tr&gt;
		&lt;/table&gt;
                 
	&lt;/div&gt;
&lt;/div&gt;

&lt;!-- &lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row-1&quot;&gt;
        &lt;div class=&quot;col-8&quot;&gt;
            &lt;p&gt;
            Implemented the biRRT-Connect algorithm to search the collision-free paths for the left arm of the PR2 robot in configuration space from the current configuration to a goal configuration, where object located.
            &lt;/p&gt;
            &lt;p&gt;
            In this project 
            &lt;/p&gt;
            &lt;ul&gt;
                &lt;li&gt;
                    control
                &lt;/li&gt;
                &lt;li&gt;
                    motion planning
                &lt;/li&gt;

            &lt;/ul&gt;
        &lt;/div&gt;

         &lt;div class=&quot;col-2&quot;&gt;
            &lt;video class=&quot;embed-responsive-item&quot; controls autoplay muted&gt;
                &lt;source src=&quot;/assets/posts_img/PR2_robot_motion_planning/pr2_rrt_demo.mp4/150x200&quot; type=&quot;video/mp4&quot;&gt;
                  Your browser does not support the video tag.
            &lt;/video&gt;
   		&lt;/div&gt;

    &lt;/div&gt;


    &lt;div class=&quot;col-4&quot;&gt;
            &lt;img class=&quot;img-fluid img-thumbnail&quot; src=&quot;/assets/posts_img/PR2_robot_motion_planning/photo.png&quot; &gt;
    &lt;/div&gt;

    &lt;hr /&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;p class=&quot;text-muted&quot;&gt;
                sth sth sht hst  
            &lt;/p&gt;
        &lt;/div&gt;

    &lt;/div&gt;

&lt;/div&gt; --&gt;</content><author><name>Xiaoyu Zhang</name></author><category term="Robotics" /><summary type="html">Developed a navigation and manipulation planner for PR2 robot that can navigate to goal pose and move one of its end-effector to given position in the given environment.</summary></entry><entry><title type="html">Autonomous Navigation Robot</title><link href="http://localhost:4000/autonomous_navigation_robot.html" rel="alternate" type="text/html" title="Autonomous Navigation Robot" /><published>2016-05-10T15:52:31+08:00</published><updated>2016-05-10T15:52:31+08:00</updated><id>http://localhost:4000/autonomous_navigation_robot</id><content type="html" xml:base="http://localhost:4000/autonomous_navigation_robot.html">&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-8&quot;&gt;
            &lt;p&gt;
            This is the first autonomous robot I built. 
            &lt;/p&gt;
            &lt;p&gt;
            The goal of this project is to navigate a course of twelve ordered and oriented waypoints under autonomous control and not collide with any of the obstacles. This project is based on the TurtleBot robot using ROS/C++.
            &lt;/p&gt;
            &lt;p&gt;
                In this system:
            &lt;/p&gt;
            &lt;ul&gt;
                &lt;li&gt;
                    Implemented the EKF SLAM algorithm that make robot to know where it is and where obstacle is after action. 
                &lt;/li&gt;
                &lt;li&gt;
                    Developed a motion planner that was capable of navigating to waypoints in space using state lattice planner.
                &lt;/li&gt;
                &lt;li&gt;
                    Implemented the measurement models based on the beam model and use laser-scanner sensor from Microsoft Kinect. 
                &lt;/li&gt;

            &lt;/ul&gt;
        &lt;/div&gt;
        &lt;div class=&quot;col-4&quot;&gt;
            &lt;img class=&quot;img-fluid img-thumbnail&quot; src=&quot;/assets/posts_img/autonomous_navigation_robot/detail.png&quot; &gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;hr /&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;p class=&quot;text-muted&quot;&gt;
                Thanks to Prof. Thomas Howard for advising  
            &lt;/p&gt;
        &lt;/div&gt;

    &lt;/div&gt;

&lt;/div&gt;</content><author><name>Xiaoyu Zhang</name></author><category term="Robotics" /><summary type="html">This is the first autonomous robot I built. The goal of this project is to navigate a course of twelve ordered and oriented waypoints under autonomous control and not collide with any of the obstacles. This project is based on the TurtleBot robot using ROS/C++. In this system: Implemented the EKF SLAM algorithm that make robot to know where it is and where obstacle is after action. Developed a motion planner that was capable of navigating to waypoints in space using state lattice planner. Implemented the measurement models based on the beam model and use laser-scanner sensor from Microsoft Kinect.</summary></entry><entry><title type="html">Depth Estimation From Single Image</title><link href="http://localhost:4000/single_image_depth_estimation.html" rel="alternate" type="text/html" title="Depth Estimation From Single Image" /><published>2015-12-01T15:52:31+08:00</published><updated>2015-12-01T15:52:31+08:00</updated><id>http://localhost:4000/single_image_depth_estimation</id><content type="html" xml:base="http://localhost:4000/single_image_depth_estimation.html">&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;p&gt;
            In this project, we aimed to solve the problem of estimating depth information from single images.
            We tested our algorithms on both Kinect style data and correlating RGB image and Lidar data.
            &lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;p&gt;
                Many sorts of deep neural networks was covered in this project, 
                but we found that the network structure from 
                &lt;a href=&quot;http://papers.nips.cc/paper/5539-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network.pdf&quot;&gt;Eigen, et.al&lt;/a&gt;
                 worked best. We finally settled down and used a similiar network structure. 
            &lt;/p&gt;

        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;img class=&quot;img-fluid&quot; src=&quot;/assets/posts_img/single_image_depth_estimation/network.png&quot; /&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;p&gt;
                And we achieved acceptable results on different datasets. 
            &lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
            &lt;div class=&quot;col-12&quot;&gt;
                &lt;table class=&quot;table&quot;&gt;
                &lt;thead&gt;
                    &lt;tr&gt;
                    &lt;th scope=&quot;col&quot;&gt;Origin Image&lt;/th&gt;
                    &lt;th scope=&quot;col&quot;&gt;Ground Truth&lt;/th&gt;
                    &lt;th scope=&quot;col&quot;&gt;Predicted Depth&lt;/th&gt;
                    &lt;/tr&gt;
                &lt;/thead&gt;
                &lt;tbody&gt;
                    &lt;tr&gt;
                    &lt;td&gt;
                            &lt;img src=&quot;/assets/posts_img/single_image_depth_estimation/0_origin_rgb.jpg&quot; class=&quot;img-thumbnail img-fluid&quot;&gt;
                    &lt;/td&gt;
                    &lt;td&gt;
                            &lt;img src=&quot;/assets/posts_img/single_image_depth_estimation/0_origin_dps.jpg&quot; class=&quot;img-thumbnail img-fluid&quot;&gt;
                    &lt;/td&gt;
                    &lt;td&gt;
                            &lt;img src=&quot;/assets/posts_img/single_image_depth_estimation/0_predicted_dps.jpg&quot; class=&quot;img-thumbnail img-fluid&quot;&gt;
                    &lt;/td&gt;
                    &lt;/tr&gt;
                &lt;/tbody&gt;
                &lt;/table&gt;
            &lt;/div&gt;
        &lt;/div&gt;



    &lt;div class=&quot;row&quot;&gt;

            &lt;div class=&quot;col-12&quot;&gt;
                &lt;p&gt;
                    We also have a &lt;a href=&quot;/assets/posts_img/single_image_depth_estimation/single-image-depth.pdf&quot; target=&quot;_blank&quot;&gt;pdf version of project report &lt;/a&gt; including different metrics. 
                &lt;/p&gt;
    
            &lt;/div&gt;
        &lt;/div&gt;


&lt;/div&gt;</content><author><name>Xiaoyu Zhang</name></author><category term="Machine Learning" /><summary type="html">In this project, we aimed to solve the problem of estimating depth information from single images. We tested our algorithms on both Kinect style data and correlating RGB image and Lidar data.</summary></entry><entry><title type="html">Vehicle Detection</title><link href="http://localhost:4000/vehicle_detection.html" rel="alternate" type="text/html" title="Vehicle Detection" /><published>2015-08-01T15:52:31+08:00</published><updated>2015-08-01T15:52:31+08:00</updated><id>http://localhost:4000/vehicle_detection</id><content type="html" xml:base="http://localhost:4000/vehicle_detection.html">&lt;div class=&quot;container&quot;&gt;
    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;p&gt;
            This project is a part of the final project of the University of Michigan EECS 599 Autonomous Driving course.
            The project was submitted, competed and rated as a
            &lt;a href=&quot;https://www.kaggle.com/c/rob599-f2017-project-task1&quot;&gt; Kaggle competition &lt;/a&gt;. Our group got 6th out of 35 different groups.
            &lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;p&gt;
                The dataset includes RGB images and correlated lidar depth information from simulated scenes.
                I guess it was from GTA5 or using similar game engine. A very frustrating thing in this project was that, only
                mid-sized cars, compact cars or SUVs were our target of detection, which means we need to distinct van and trucks,
                and do not report them in our results.
            &lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
            &lt;div class=&quot;col-12&quot;&gt;
                &lt;p&gt;
                    We transformed the lidar information
                    to the same size of a layer of the RGB image using bilateral filter ,
                    and fine tuned the network from pretrained yolo-v2 from darknet.
                &lt;/p&gt;
            &lt;/div&gt;
        &lt;/div&gt;

    &lt;div class=&quot;row&quot;&gt;
        &lt;div class=&quot;col-12&quot;&gt;
            &lt;img class=&quot;img-fluid&quot; src=&quot;/assets/posts_img/vehicle_detection/predictions.jpg&quot; /&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&quot;row&quot;&gt;
            &lt;div class=&quot;col-12&quot;&gt;
                &lt;p&gt;
                    You can also find our &lt;a href=&quot;/assets/posts_img/vehicle_detection/599_report.pdf&quot; target=&quot;_blank&quot;&gt;pdf report &lt;/a&gt; in which we reported the details of the project.
                &lt;/p&gt;

            &lt;/div&gt;
        &lt;/div&gt;
&lt;/div&gt;</content><author><name>Xiaoyu Zhang</name></author><category term="Machine Learning" /><summary type="html">This project is a part of the final project of the University of Michigan EECS 599 Autonomous Driving course. The project was submitted, competed and rated as a Kaggle competition . Our group got 6th out of 35 different groups.</summary></entry></feed>